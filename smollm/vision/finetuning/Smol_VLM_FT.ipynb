{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/merveenoyan/smollm/blob/main/vision/finetuning/Smol_VLM_FT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nc0g2NLpUSGr"
   },
   "source": [
    "# Fine-tune SmolVLM on Visual Question Answering using Consumer GPU with QLoRA\n",
    "\n",
    "In this notebook we will fine-tune SmolVLM VQAv2 dataset. With this notebook you can also fine-tune Idefics3, since both models have the same model class/architecture.\n",
    "\n",
    "We will use some techniques in this notebook that will let you fine-tune the model on L4 with batch size of 4 only using around 16.4 GB of VRAM. We ran this notebook in that setup to test, but because we were able to afford A100 this notebook was last ran on an A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIhA1lQ7j0kw"
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate datasets peft bitsandbytes tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyJaqZZ3uYYl"
   },
   "outputs": [],
   "source": [
    "!pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAeMA0heVBjT"
   },
   "source": [
    "We will push out model to Hub so we need to authenticate ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yKd5xtSGj7cm",
    "ExecuteTime": {
     "end_time": "2025-04-11T07:25:03.457248Z",
     "start_time": "2025-04-11T07:25:03.336390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a73e5c1c56244e087ad71edd01057cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRq8ve-LVAzU"
   },
   "source": [
    "In this notebook we will not do full fine-tuning but use QLoRA method, which loads an adapter to the quantized version of the model, saving space. If you want to do full fine-tuning, set `USE_LORA` and `USE_QLORA` to False. If you want to do LoRA, set `USE_QLORA` to False and `USE_LORA` to True."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'clear_device_cache' from 'accelerate.utils.memory' (/mnt/share/toky/CondaEnvs/LM/lib/python3.10/site-packages/accelerate/utils/memory.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpeft\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoProcessor, BitsAndBytesConfig, Idefics3ForConditionalGeneration\n\u001B[1;32m      5\u001B[0m USE_LORA \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/mnt/share/toky/CondaEnvs/LM/lib/python3.10/site-packages/peft/__init__.py:17\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     15\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.15.1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     18\u001B[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001B[1;32m     19\u001B[0m     AutoPeftModel,\n\u001B[1;32m     20\u001B[0m     AutoPeftModelForCausalLM,\n\u001B[1;32m     21\u001B[0m     AutoPeftModelForFeatureExtraction,\n\u001B[1;32m     22\u001B[0m     AutoPeftModelForQuestionAnswering,\n\u001B[1;32m     23\u001B[0m     AutoPeftModelForSeq2SeqLM,\n\u001B[1;32m     24\u001B[0m     AutoPeftModelForSequenceClassification,\n\u001B[1;32m     25\u001B[0m     AutoPeftModelForTokenClassification,\n\u001B[1;32m     26\u001B[0m )\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PeftConfig, PromptLearningConfig\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmapping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     29\u001B[0m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001B[1;32m     30\u001B[0m     PEFT_TYPE_TO_MIXED_MODEL_MAPPING,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     33\u001B[0m     inject_adapter_in_model,\n\u001B[1;32m     34\u001B[0m )\n",
      "File \u001B[0;32m/mnt/share/toky/CondaEnvs/LM/lib/python3.10/site-packages/peft/auto.py:31\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Optional\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     22\u001B[0m     AutoModel,\n\u001B[1;32m     23\u001B[0m     AutoModelForCausalLM,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     28\u001B[0m     AutoTokenizer,\n\u001B[1;32m     29\u001B[0m )\n\u001B[0;32m---> 31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PeftConfig\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpeft_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     33\u001B[0m     PeftModel,\n\u001B[1;32m     34\u001B[0m     PeftModelForCausalLM,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     39\u001B[0m     PeftModelForTokenClassification,\n\u001B[1;32m     40\u001B[0m )\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TOKENIZER_CONFIG_NAME\n",
      "File \u001B[0;32m/mnt/share/toky/CondaEnvs/LM/lib/python3.10/site-packages/peft/config.py:24\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m hf_hub_download\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PushToHubMixin\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CONFIG_NAME, PeftType, TaskType\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# we expect at least these keys to be present in a PEFT adapter_config.json\u001B[39;00m\n\u001B[1;32m     28\u001B[0m MIN_EXPECTED_CONFIG_KEYS \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpeft_type\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n",
      "File \u001B[0;32m/mnt/share/toky/CondaEnvs/LM/lib/python3.10/site-packages/peft/utils/__init__.py:16\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegrations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m map_cache_to_layer_device_map\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloftq_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m replace_lora_weights_loftq\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mother\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     18\u001B[0m     CONFIG_NAME,\n\u001B[1;32m     19\u001B[0m     INCLUDE_LINEAR_LAYERS_SHORTHAND,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     50\u001B[0m     transpose,\n\u001B[1;32m     51\u001B[0m )\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpeft_types\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PeftType, TaskType, register_peft_method\n",
      "File \u001B[0;32m/mnt/share/toky/CondaEnvs/LM/lib/python3.10/site-packages/peft/utils/loftq_utils.py:25\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Callable, Optional, Union\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maccelerate\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmemory\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m clear_device_cache\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m snapshot_download\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HFValidationError, LocalEntryNotFoundError\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'clear_device_cache' from 'accelerate.utils.memory' (/mnt/share/toky/CondaEnvs/LM/lib/python3.10/site-packages/accelerate/utils/memory.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, Idefics3ForConditionalGeneration\n",
    "\n",
    "USE_LORA = False\n",
    "USE_QLORA = True  #配置低比特量化参数\n",
    "SMOL = True\n",
    "\n",
    "model_id = \"HuggingFaceTB/SmolVLM-Base\" if SMOL else \"HuggingFaceM4/Idefics3-8B-Llama3\"\n",
    "print(\"1\")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "print(\"2\")\n",
    "if USE_QLORA or USE_LORA:\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=['down_proj', 'o_proj', 'k_proj', 'q_proj', 'gate_proj', 'up_proj', 'v_proj'],\n",
    "        use_dora=False if USE_QLORA else True,\n",
    "        init_lora_weights=\"gaussian\"\n",
    "    )  #定义 LoRA 的配置参数。r表示低秩矩阵的秩，lora_alpha是缩放因子，lora_dropout是 Dropout 概率，target_modules指定要应用 LoRA 的模块，use_dora是一个与 Dora 优化相关的选项（根据USE_QLORA的值进行设置），init_lora_weights指定初始化 LoRA 权重的方式。\n",
    "    lora_config.inference_mode = False  #设置 LoRA 配置为非推理模式，意味着模型处于训练准备状态。\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )  #定义低比特量化的配置。load_in_4bit表示以 4 比特精度加载模型，bnb_4bit_use_double_quant启用双重量化，bnb_4bit_quant_type指定量化类型为\"nf4\"，bnb_4bit_compute_dtype指定计算数据类型为torch.bfloat16。\n",
    "\n",
    "    model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config if USE_QLORA else None,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\"\n",
    "    )  #从预训练模型加载 Idefics3ForConditionalGeneration 模型，并根据 USE_QLORA 的值配置 量化参数，设置注意力机制的实现方式为 \"flash_attention_2\"，并自动分配设备。\n",
    "    model.add_adapter(lora_config)  #为模型添加 LoRA 适配器。\n",
    "    model.enable_adapters()\n",
    "    model = prepare_model_for_kbit_training(model)  #为低比特训练准备模型。\n",
    "    model = get_peft_model(model, lora_config)  #获取经过 LoRA 优化的模型。\n",
    "    print(model.get_nb_trainable_parameters())\n",
    "else:\n",
    "    model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "    ).to(DEVICE) #从预训练模型加载Idefics3ForConditionalGeneration模型，设置数据类型为torch.bfloat16，注意力机制为\"flash_attention_2\"，并将模型移动到指定设备（代码中DEVICE未定义，需根据实际情况确定）。\n",
    "\n",
    "    # if you'd like to only fine-tune LLM\n",
    "    for param in model.model.vision_model.parameters():\n",
    "        param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-11T07:56:47.930740Z",
     "start_time": "2025-04-11T07:56:44.995797Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIVhpp0EyZO2"
   },
   "source": [
    "The model as is is holding 2.7 GB of GPU RAM 💗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMTtg3dl3NX2"
   },
   "source": [
    "## Loading the dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWHMWTSZ3Pyr"
   },
   "source": [
    "We will load a small portion of the VQAv2 dataset. We are loading a small portion of the model for education purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POOqKqYRka5O"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('merve/vqav2-small', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Znf9vMo5rnSd"
   },
   "outputs": [],
   "source": [
    "split_ds = ds[\"validation\"].train_test_split(test_size=0.5)\n",
    "train_ds = split_ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIDioFlRuYYn",
    "outputId": "79b697a7-d245-4fdc-b0e8-d9ffa8627953"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['multiple_choice_answer', 'question', 'image'],\n",
       "    num_rows: 10717\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nwMO3n0X7Hv"
   },
   "source": [
    "Let's write our data collating function. We will apply prompt template to have questions and answers together so model can learn to answer. Then we pass the formatted prompts and images to the processor which processes both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0krVLZ-wNMl"
   },
   "outputs": [],
   "source": [
    "image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "    processor.tokenizer.additional_special_tokens.index(\"<image>\")]\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        image = example[\"image\"]\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        question = example[\"question\"]\n",
    "        answer = example[\"multiple_choice_answer\"]\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Answer briefly.\"},\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": question}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": answer}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "        texts.append(text.strip())\n",
    "        images.append([image])\n",
    "\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEYDjWpE3LD5"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvAs896cdwg8"
   },
   "source": [
    "We can now initialize `Trainer` and initialize `TrainingArguments` to pass to `Trainer`.\n",
    "\n",
    "Some notes:\n",
    "- If you use 8-bit QLoRA with the below setup it uses around 16.4 GB VRAM (beautiful, fits comfortably inside L4, Colab free tier)\n",
    "- We use gradient accumulation to simulate a larger batch size.\n",
    "- We also save up on memory from intermediate activations by using gradient checkpointing.\n",
    "\n",
    "**Disclaimer:**\n",
    "The techniques here aren't free lunch. The latter two will add additional compute to the training, thus slow down a bit (for reference on two A100s with bsz of 16, we were able to train for 2 hrs 43 mins with the gradient accumulation steps of 4, disabling it reduced it with 2 hr 35 mins).\n",
    "If you want to speed-up, you might play around, reduce to 4-bit precision and have a higher batch size. Note that 4-bit might result in model learning less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNE2yWAYrAhD"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    save_total_limit=1,\n",
    "    optim=\"paged_adamw_8bit\",  # for 8-bit, keep this, else adamw_hf\n",
    "    bf16=True,  # underlying precision for 8bit\n",
    "    output_dir=f\"./{model_name}-vqav2\",\n",
    "    hub_model_id=f\"{model_name}-vqav2\",\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBBSDpBhreJd",
    "outputId": "071ed677-1d9f-4f98-9d19-64834440c9c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QOCpw_-uYYo",
    "outputId": "7abb6937-c072-435a-c3f5-6dbb5b0b9eea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9/670 01:41 < 2:39:41, 0.07 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hN0QD9_uYYo"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "name": "Smol_VLM_FT.ipynb",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
