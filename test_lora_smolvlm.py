import torch
import json
import os
from PIL import Image
from transformers import AutoProcessor, AutoModelForImageTextToText
from peft import PeftModel

# -------------------------- 1. æµ‹è¯•é…ç½®ï¼ˆéœ€æ ¹æ®ä½ çš„è·¯å¾„ä¿®æ”¹ï¼‰ --------------------------
# ä¸è®­ç»ƒä¸€è‡´çš„å…³é”®å‚æ•°ï¼ˆå¿…é¡»å’Œè®­ç»ƒä»£ç å®Œå…¨ç›¸åŒï¼‰
IMAGE_FIELD_TEST = "image"  # æµ‹è¯•é›†JSONLä¸­å›¾åƒè·¯å¾„çš„å­—æ®µåï¼ˆå¯¹åº”è®­ç»ƒçš„IMAGE_FIELD="frame_path"ï¼‰
NUM_IMAGE_MARKS = 81  # ä¸è®­ç»ƒä¸€è‡´ï¼š81ä¸ª<image>æ ‡è®°
MAX_TEXT_LENGTH = 1024 + NUM_IMAGE_MARKS  # ä¸è®­ç»ƒä¸€è‡´ï¼š1024æ–‡æœ¬Token + 81å›¾åƒToken

# è·¯å¾„é…ç½®
BASE_MODEL_PATH = "/mnt/share/HuggingfaceModels/HuggingFaceTB/SmolVLM2-2.2B-Instruct"  # åŒè®­ç»ƒ
LORA_MODEL_PATH = "smolvlm2-lora-final-20250904"  # è®­ç»ƒå¥½çš„LoRAæ¨¡å‹è·¯å¾„
TEST_DATA_PATH = "/media/user/data3/toky/Projects/Evaluate-Surgery-VLM/split_data/generated_long_questions_with_response.jsonl"  # æµ‹è¯•é›†è·¯å¾„
OUTPUT_PATH = "smolvlm_test_results.jsonl"  # æµ‹è¯•ç»“æœè¾“å‡ºè·¯å¾„


# -------------------------- 2. åŠ è½½æµ‹è¯•ç”¨æ¨¡å‹/å¤„ç†å™¨ï¼ˆä»…åŠ è½½ï¼Œä¸è®­ç»ƒï¼‰ --------------------------
def load_test_model_processor(base_path, lora_path):
    """åŠ è½½è®­ç»ƒå¥½çš„LoRAæ¨¡å‹+å¤„ç†å™¨ï¼ˆå®Œå…¨å¯¹é½è®­ç»ƒæ—¶çš„é…ç½®ï¼‰"""
    # 1. åŠ è½½å¤„ç†å™¨ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼štrust_remote_code=True + pad_tokenè®¾ç½®ï¼‰
    processor = AutoProcessor.from_pretrained(
        base_path,
        local_files_only=True,
        trust_remote_code=True
    )
    if processor.tokenizer.pad_token is None:
        processor.tokenizer.pad_token = processor.tokenizer.eos_token  # åŒè®­ç»ƒ
        print("âœ… å·²è®¾ç½®pad_tokenä¸ºeos_tokenï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰")

    # 2. åŠ è½½Baseæ¨¡å‹ï¼ˆä¸è®­ç»ƒä¸€è‡´çš„é‡åŒ–é…ç½®ï¼‰
    model = AutoModelForImageTextToText.from_pretrained(
        base_path,
        quantization_config=dict(  # å¤åˆ»è®­ç»ƒæ—¶çš„BitsAndBytesConfig
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
        ),
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    )

    # 3. èåˆLoRAæƒé‡ï¼ˆæµ‹è¯•æ—¶ä»…åŠ è½½ï¼Œä¸è®­ç»ƒï¼‰
    model = PeftModel.from_pretrained(
        model=model,
        model_id=lora_path,
        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    )
    model.eval()  # æµ‹è¯•å…³é”®ï¼šåˆ‡æ¢ä¸ºæ¨ç†æ¨¡å¼ï¼ˆç¦ç”¨Dropoutï¼‰
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ˆBase+LoRAï¼Œæ¨ç†æ¨¡å¼ï¼‰")
    return model, processor


# -------------------------- 3. è§£ææµ‹è¯•é›†ï¼ˆä»…è¯»å–æµ‹è¯•æ•°æ®ï¼‰ --------------------------
def parse_test_data(data_path):
    """è§£ææµ‹è¯•é›†JSONLï¼Œè¿‡æ»¤æ— æ•ˆæ ·æœ¬ï¼ˆä¸è®­ç»ƒä¸€è‡´çš„å­—æ®µæ£€æŸ¥ï¼‰"""
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"æµ‹è¯•é›†ä¸å­˜åœ¨ï¼š{data_path}")

    valid_samples = []
    with open(data_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            try:
                sample = json.loads(line.strip())
                # ä¸è®­ç»ƒä¸€è‡´ï¼šæ£€æŸ¥å¿…è¦å­—æ®µï¼ˆæµ‹è¯•é›†ç”¨"text"å¯¹åº”è®­ç»ƒçš„"prompt"ï¼‰
                required_fields = [IMAGE_FIELD_TEST, "text", "response"]
                if not all(f in sample for f in required_fields):
                    print(f"âš ï¸  ç¬¬{line_num}è¡Œæ ·æœ¬ç¼ºå°‘å­—æ®µï¼Œè·³è¿‡")
                    continue
                valid_samples.append(sample)
            except json.JSONDecodeError:
                print(f"âš ï¸  ç¬¬{line_num}è¡ŒJSONæ ¼å¼é”™è¯¯ï¼Œè·³è¿‡")
                continue
    print(f"âœ… è§£æå®Œæˆï¼šå…±{len(valid_samples)}ä¸ªæœ‰æ•ˆæµ‹è¯•æ ·æœ¬")
    return valid_samples


# -------------------------- 4. æµ‹è¯•æ ·æœ¬é¢„å¤„ç†ï¼ˆ100%å¤åˆ»è®­ç»ƒçš„preprocess_single_sampleï¼‰ --------------------------
def preprocess_test_sample(sample, processor):
    img_path = sample[IMAGE_FIELD_TEST]
    prompt_text = sample["text"]
    question_id = sample.get("question_id", f"sample_{id(sample)}")

    # -------------------------- å›¾åƒå¤„ç†ï¼ˆæ— ä¿®æ”¹ï¼‰ --------------------------
    if not os.path.exists(img_path):
        raise FileNotFoundError(f"æ ·æœ¬{question_id}ï¼šå›¾åƒä¸å­˜åœ¨ï¼š{img_path}")
    try:
        with Image.open(img_path) as img:
            if hasattr(img, 'n_frames') and img.n_frames > 1:
                img.seek(0)
            image = img.convert("RGB")
    except Exception as e:
        raise RuntimeError(f"æ ·æœ¬{question_id}ï¼šåŠ è½½å›¾åƒå¤±è´¥ï¼š{str(e)}") from e

    image_inputs = processor.image_processor(image, return_tensors="pt")
    pixel_values = image_inputs["pixel_values"].squeeze(0)
    pixel_values = pixel_values[0, :, :, :]
    pixel_values = pixel_values.unsqueeze(0)
    assert pixel_values.shape == (1, 3, 384, 384), \
        f"æ ·æœ¬{question_id}ï¼šå›¾åƒå½¢çŠ¶é”™è¯¯ï¼éœ€(1,3,384,384)ï¼Œå½“å‰{pixel_values.shape}"

    # -------------------------- æ–‡æœ¬å¤„ç†ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼šé¿å…<image>è¢«åˆå¹¶ï¼‰ --------------------------
    # 1. æ¸…ç†promptï¼šç§»é™¤æ‰€æœ‰å¯èƒ½ç ´å<image>çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆæ¯”ä¹‹å‰æ›´å½»åº•ï¼‰
    clean_prompt = prompt_text.replace("<image>", "").replace("\r", "").strip()
    # æ–°å¢ï¼šç§»é™¤æ‰€æœ‰éå­—æ¯æ•°å­—/ä¸­æ–‡çš„å­—ç¬¦ï¼Œé¿å…å¹²æ‰°<image>æ ‡è®°
    clean_prompt = ''.join([c for c in clean_prompt if c.isalnum() or c.isspace() or '\u4e00' <= c <= '\u9fff'])

    # 2. ç”Ÿæˆ<image>æ ‡è®°ï¼šç”¨ç©ºæ ¼åˆ†éš”æ¯ä¸ª<image>ï¼Œé¿å…Tokenizeråˆå¹¶è¿ç»­ç›¸åŒToken
    # å…³é”®ï¼šè®­ç»ƒæ—¶æ–‡æœ¬æ˜¯è¿ç»­<image>ï¼Œä½†æµ‹è¯•æ—¶è¿ç»­å¯èƒ½è¢«åˆå¹¶ï¼Œç”¨ç©ºæ ¼åˆ†éš”ç¡®ä¿æ¯ä¸ªéƒ½è¢«è¯†åˆ«
    num_image_marks = NUM_IMAGE_MARKS
    image_tokens = " <image> " * num_image_marks  # æ¯ä¸ª<image>å‰ååŠ ç©ºæ ¼
    image_tokens = image_tokens.strip()  # å»é™¤é¦–å°¾ç©ºæ ¼ï¼Œé¿å…å¤šä½™Token

    # 3. æ„å»ºæ–‡æœ¬ï¼šä¿æŒæ ¼å¼å¯¹é½ï¼ˆç©ºæ ¼åˆ†éš”çš„<image> + æ¸…ç†åçš„promptï¼‰
    text = f"{image_tokens}\n{clean_prompt}"

    # 4. éªŒè¯ç¼–ç å‰<image>æ•°é‡ï¼ˆç¡®ä¿ç”Ÿæˆæ­£ç¡®ï¼‰
    current_image_count = text.count("<image>")
    if current_image_count != num_image_marks:
        raise ValueError(
            f"æ ·æœ¬{question_id}ï¼šç¼–ç å‰<image>æ•°é‡é”™è¯¯ï¼éœ€{num_image_marks}ä¸ªï¼Œå½“å‰{current_image_count}ä¸ª"
        )

    # 5. æ–‡æœ¬ç¼–ç ï¼ˆåŒè®­ç»ƒå‚æ•°ï¼‰
    text_inputs = processor.tokenizer(
        text,
        truncation=True,
        max_length=MAX_TEXT_LENGTH,
        padding="max_length",
        return_tensors="pt",
        return_attention_mask=True
    )

    # -------------------------- æ–°å¢ï¼šå¼ºåˆ¶éªŒè¯ç¼–ç å<image>æ•°é‡ï¼ˆæ ¸å¿ƒæ‹¦æˆªé”™è¯¯ï¼‰ --------------------------
    # è·å–<image>çš„Token IDï¼ˆæ¯æ¬¡éƒ½é‡æ–°è·å–ï¼Œé¿å…æ˜ å°„å˜åŒ–ï¼‰
    image_token_id = processor.tokenizer.convert_tokens_to_ids("<image>")
    if image_token_id == processor.tokenizer.unk_token_id:
        raise ValueError(f"æ ·æœ¬{question_id}ï¼š<image>ä¸æ˜¯æœ‰æ•ˆTokenï¼ˆæœªçŸ¥Token IDï¼‰")

    # ç»Ÿè®¡ç¼–ç å<image>çš„æ•°é‡
    encoded_image_count = (text_inputs["input_ids"] == image_token_id).sum().item()
    if encoded_image_count != num_image_marks:
        raise ValueError(
            f"æ ·æœ¬{question_id}ï¼šç¼–ç å<image>ä¸¢å¤±ï¼ç¼–ç å‰={current_image_count}ä¸ªï¼Œç¼–ç å={encoded_image_count}ä¸ª"
            f"\n  æ–‡æœ¬åŸæ–‡å‰100å­—ç¬¦ï¼š{text[:100]}"  # æ‰“å°éƒ¨åˆ†åŸæ–‡ï¼Œå¸®åŠ©å®šä½é—®é¢˜
            f"\n  ç¼–ç åå‰20ä¸ªToken IDï¼š{text_inputs['input_ids'][0][:20]}"  # æ‰“å°éƒ¨åˆ†Token IDï¼ŒæŸ¥çœ‹æ˜¯å¦åˆå¹¶
        )

    # åç»­å¤„ç†ï¼ˆæ— ä¿®æ”¹ï¼‰
    input_ids = text_inputs["input_ids"].squeeze(0)
    attention_mask = text_inputs["attention_mask"].squeeze(0)
    assert input_ids.shape == (MAX_TEXT_LENGTH,), \
        f"æ ·æœ¬{question_id}ï¼šæ–‡æœ¬é•¿åº¦é”™è¯¯ï¼éœ€{MAX_TEXT_LENGTH}ï¼Œå½“å‰{input_ids.shape[0]}"

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "pixel_values": pixel_values,
        "sample_info": {
            "question_id": question_id,
            "image_path": img_path,
            "original_prompt": prompt_text,
            "ground_truth": sample["response"],
            "encoded_image_count": encoded_image_count  # è®°å½•ç¼–ç åæ•°é‡ï¼Œä¾¿äºè°ƒè¯•
        }
    }

# -------------------------- 5. æ‰§è¡Œæµ‹è¯•æ¨ç†ï¼ˆæ ¸å¿ƒï¼šä»…æµ‹è¯•ï¼Œæ— è®­ç»ƒï¼‰ --------------------------
def run_test_inference(model, processor, test_samples):
    device = model.device
    model_dtype = model.dtype
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯•æ¨ç†ï¼ˆå…±{len(test_samples)}ä¸ªæ ·æœ¬ï¼Œè®¾å¤‡ï¼š{device}ï¼Œæ¨¡å‹ç±»å‹ï¼š{model_dtype}ï¼‰")

    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        for idx, sample in enumerate(test_samples, 1):
            # ç”¨æ ·æœ¬è‡ªå¸¦çš„question_idï¼Œé¿å…æœªçŸ¥ID
            question_id = sample.get("question_id", f"unknown_{idx}")
            try:
                # -------------------------- å…³é”®ï¼šé¢„å¤„ç†æ—¶å·²éªŒè¯ç¼–ç å<image>æ•°é‡ä¸º81 --------------------------
                preprocessed = preprocess_test_sample(sample, processor)
                # æ‰“å°ç¼–ç å<image>æ•°é‡ï¼Œç¡®è®¤æ­£å¸¸
                print(f"â„¹ï¸  æ ·æœ¬{idx}ï¼ˆID:{question_id}ï¼‰ï¼šç¼–ç å<image>æ•°é‡={preprocessed['sample_info']['encoded_image_count']}")

                # ç±»å‹è½¬æ¢ä¸æ¨ç†ï¼ˆæ— ä¿®æ”¹ï¼‰
                input_ids = preprocessed["input_ids"].to(device)
                attention_mask = preprocessed["attention_mask"].to(device)
                pixel_values = preprocessed["pixel_values"].to(device, dtype=model_dtype)

                with torch.no_grad():
                    # ä¿®å¤è­¦å‘Šï¼šdo_sample=Falseæ—¶æ— éœ€è®¾ç½®temperatureï¼Œç›´æ¥åˆ é™¤temperatureå‚æ•°
                    outputs = model.generate(
                        input_ids=input_ids.unsqueeze(0),
                        attention_mask=attention_mask.unsqueeze(0),
                        pixel_values=pixel_values.unsqueeze(0),
                        max_new_tokens=128,
                        do_sample=False  # åˆ é™¤temperature=0.0ï¼Œé¿å…è­¦å‘Š
                    )

                # è§£ç ä¸ä¿å­˜ï¼ˆæ— ä¿®æ”¹ï¼‰
                model_response = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)
                input_prefix = f"{(' <image> ' * NUM_IMAGE_MARKS).strip()}\n{preprocessed['sample_info']['original_prompt'].replace('<image>', '').strip()}"
                if model_response.startswith(input_prefix):
                    model_response = model_response[len(input_prefix):].strip()

                result = {
                    "question_id": preprocessed["sample_info"]["question_id"],
                    "image_path": preprocessed["sample_info"]["image_path"],
                    "original_prompt": preprocessed["sample_info"]["original_prompt"],
                    "ground_truth": preprocessed["sample_info"]["ground_truth"],
                    "model_response": model_response,
                    "status": "success"
                }
                f.write(json.dumps(result, ensure_ascii=False) + "\n")
                print(f"âœ… å®Œæˆ {idx}/{len(test_samples)} | ID: {question_id}")

            except Exception as e:
                # é”™è¯¯æ—¥å¿—ä¸­åŒ…å«ç¼–ç å<image>æ•°é‡ï¼ˆå¦‚æœå·²é¢„å¤„ç†ï¼‰
                encoded_count = preprocessed["sample_info"]["encoded_image_count"] if 'preprocessed' in locals() else "æœªçŸ¥"
                error_result = {
                    "question_id": question_id,
                    "image_path": sample.get(IMAGE_FIELD_TEST, ""),
                    "original_prompt": sample.get("text", "")[:50] + "..." if sample.get("text") else "",  # æ‰“å°éƒ¨åˆ†prompt
                    "ground_truth": sample.get("response", "")[:30] + "..." if sample.get("response") else "",
                    "model_response": "",
                    "status": "failed",
                    "encoded_image_count": encoded_count,  # è®°å½•ç¼–ç åæ•°é‡
                    "error": str(e)
                }
                f.write(json.dumps(error_result, ensure_ascii=False) + "\n")
                print(f"âŒ å¤±è´¥ {idx}/{len(test_samples)} | ID: {question_id} | ç¼–ç å<image>æ•°ï¼š{encoded_count} | é”™è¯¯ï¼š{str(e)}")

    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼ç»“æœä¿å­˜è‡³ï¼š{os.path.abspath(OUTPUT_PATH)}")

# -------------------------- 6. æµ‹è¯•å…¥å£ï¼ˆä»…æ‰§è¡Œæµ‹è¯•æµç¨‹ï¼‰ --------------------------
if __name__ == "__main__":
    try:
        # 1. åŠ è½½æ¨¡å‹/å¤„ç†å™¨
        model, processor = load_test_model_processor(BASE_MODEL_PATH, LORA_MODEL_PATH)
        # 2. è§£ææµ‹è¯•æ•°æ®
        test_samples = parse_test_data(TEST_DATA_PATH)
        # 3. æ‰§è¡Œæµ‹è¯•æ¨ç†
        run_test_inference(model, processor, test_samples)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¯åŠ¨å¤±è´¥ï¼š{str(e)}")